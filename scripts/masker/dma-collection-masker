#!/usr/bin/env python3
# Copyright 2024 Google LLC

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     https://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from __future__ import annotations

import argparse
import csv
import logging
import random
import shutil
import string
import sys
import tarfile
import zipfile as zf
from dataclasses import dataclass
from enum import Flag, auto
from pathlib import Path
from tempfile import NamedTemporaryFile, TemporaryDirectory
from typing import Any, Dict, List, Tuple

from packaging.version import Version
from packaging.version import parse as parse_version

__all__ = (
    "ApplicationError",
    "DataMaskRule",
    "main",
    "run_masker",
)

__version__ = "4.3.39"

logger = logging.getLogger(__name__)

here = Path(__file__).parent


@dataclass
class DataMaskRule:
    mask_type: DataMaskTypes
    table_name: str
    column_position: int
    missing_ok: bool
    fake_function: Any


@dataclass
class FileMetadata:
    script_version: str
    host_name: str
    database_name: str
    instance_name: str
    date_time: str
    pkey: str
    collection_key: str


class DMAFaker:
    """Fake data provider to provide masked values"""

    hostname_prefixes: List[str] = [
        "db",
        "srv",
        "desktop",
        "laptop",
        "lt",
        "email",
        "web",
    ]

    hostname_domains: List[str] = [
        "com",
        "com",
        "com",
        "com",
        "com",
        "com",
        "biz",
        "info",
        "net",
        "org",
    ]

    def __init__(self) -> None:
        self._collection_key: str
        self._date_time: str
        self._dbid: str = self._digits(length=10)

    def _letters(self, length: int) -> str:
        return "".join(random.choices(string.ascii_lowercase, k=length))

    def _digits(self, length: int) -> str:
        return "".join(random.choices(string.digits, k=length))

    @property
    def date_time(self) -> str:
        return self._date_time

    @date_time.setter
    def date_time(self, value: str) -> None:
        self._date_time = value

    @property
    def collection_key(self) -> str:
        return self._collection_key

    def set_collection_key_from_date_time(self, date_time: str) -> None:
        if not self._date_time:
            raise ValueError("date_time is not set")
        self._collection_key = f"{self.hostname()}_{self.database_name()}_{self.instance_name()}_{date_time}"

    def database_name(self) -> str:
        return f"""db-{self._letters(length=8)}""".upper()

    def instance_name(self) -> str:
        return f"""inst-{self._letters(length=8)}""".upper()

    def hostname(self) -> str:
        prefix = f"""{random.choice(self.hostname_prefixes)}-{self._digits(length=2)}"""
        domain = f"""{self._letters(length=5)}.{random.choice(self.hostname_domains)}"""
        return f"{prefix}.{domain}"

    def pkey(self) -> str:
        return f"{self.hostname()}_{self.database_name()}_{self.date_time}"

    def dma_source_id(self) -> str:
        return f"{self.hostname()}_{self.instance_name()}_{self._dbid}"

    def user_name(self) -> str:
        return f"USER_{self._letters(length=4)}{self._digits(length=2)}".upper()


fake = DMAFaker()


class DataMaskTypes(Flag):
    SCHEMA_NAME = auto()
    PKEY = auto()
    DMA_SOURCE_ID = auto()
    INSTANCE_NAME = auto()
    HOST_NAME = auto()
    DATABASE_NAME = auto()


DATA_MASK_CONFIG = [
    DataMaskRule(
        mask_type=DataMaskTypes.PKEY, table_name="*", column_position=0, missing_ok=False, fake_function=fake.pkey
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.DMA_SOURCE_ID,
        table_name="*",
        column_position=-2,
        missing_ok=False,
        fake_function=fake.dma_source_id,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="columntypes",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="compressbytype",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="datatypes",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="dbobjectnames",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="dbobjects",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="dtlsourcecode",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="exttab",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="indextypedtl",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="indextypes",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="lobsizing",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="mviewtypes",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="sourcecode",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="tableconstraints",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="tablesnopk",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="tabletypedtl",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="tabletypes",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="triggers",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="usedspacedetails",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="usrsegatt",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.SCHEMA_NAME,
        table_name="users",
        column_position=2,
        missing_ok=True,
        fake_function=fake.user_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.INSTANCE_NAME,
        table_name="dbinstances",
        column_position=2,
        missing_ok=True,
        fake_function=fake.instance_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.HOST_NAME,
        table_name="dbinstances",
        column_position=3,
        missing_ok=True,
        fake_function=fake.hostname,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.DATABASE_NAME,
        table_name="dbsummary",
        column_position=2,
        missing_ok=True,
        fake_function=fake.database_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.DATABASE_NAME,
        table_name="dbsummary",
        column_position=-3,
        missing_ok=True,
        fake_function=fake.database_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.DATABASE_NAME,
        table_name="pdbsinfo",
        column_position=3,
        missing_ok=True,
        fake_function=fake.database_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.DATABASE_NAME,
        table_name="pdbsopenmode",
        column_position=2,
        missing_ok=True,
        fake_function=fake.database_name,
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.HOST_NAME,
        table_name="sourceconn",
        column_position=6,
        missing_ok=True,
        fake_function=fake.hostname,
    ),
]


class ApplicationError(Exception):
    """Application Error

    Raised when any error occurs with the masking process.
    """


def run_masker(input_dir: Path | None = None, output_path: Path | None = None) -> None:
    """Run Masker.

    Args:
        input_dir (Optional[Path]): Path containing collections to mask. Defaults to None.
        output_path (Optional[Path]): Path to write masked collection. Defaults to None.
    """
    with TemporaryDirectory() as temp_dir:
        work_dir = Path(temp_dir)
        work_dir.mkdir(exist_ok=True)
        input_dir = input_dir or Path("input")
        output_path = output_path or Path("masked")

        for dir_name in (input_dir, output_path):
            dir_name.mkdir(parents=True, exist_ok=True)

        collections = _find_collections_to_process(input_dir)
        if len(collections) == 0:
            logger.info("No collections found in location %s", str(input_dir))
            logger.info("Exiting...")
            sys.exit(1)

        for collection in collections:
            collection_metadata = _metadata_from_filename(collection)
            fake.date_time = collection_metadata.date_time
            fake.set_collection_key_from_date_time(collection_metadata.date_time)
            identity_map: dict[str, str] = {collection_metadata.pkey: fake.pkey()}
            _extract_collection(collection, work_dir)
            for rule in DATA_MASK_CONFIG:
                if rule.mask_type == DataMaskTypes.DMA_SOURCE_ID and Version(
                    collection_metadata.script_version
                ) < Version("4.3.15"):
                    continue
                collection_files = _find_files_referenced_by_rule(rule, work_dir)
                if collection_files:
                    for collection_file in collection_files:  # type: ignore[attr-defined]
                        identity_map = _generate_identity_map_from_rule(rule, collection_file, identity_map)
                        _apply_identity_map_to_file(rule, collection_file, identity_map)
            logger.info("Completed work on %s", collection.stem)
            _collection_key_file, _new_collection_archive = _package_collection(
                collection, identity_map, work_dir, output_path
            )
            _clean_folder(work_dir)

        logger.info("------------------------------------------------------------------------------------------")
        logger.info("------------------------------------------------------------------------------------------")
        logger.info(
            "Masking complete.  Please submit the files in the '%s' directory.",
            output_path,
        )
        logger.info(
            "Retain the *key files in the '%s' directory to map data from the assessment report to the original values.",
            input_dir,
        )


def _metadata_from_filename(collection_file: Path) -> FileMetadata:
    """Return metadata by parsing the collection file name.

    Args:
        collection_file (Path): The path of the file to read.

    Raises:
        ApplicationError: Raised when parsing metadata fails.

    Returns:
        FileMetadata: namedtuple containing metadata attributes.
    """
    try:
        meta_values = collection_file.stem.split("__", maxsplit=2)[-1].split("_")[1:6]
        meta_values[0] = str(parse_version(meta_values[0]))  # type: ignore[call-overload]
        meta_values.append("_".join(meta_values[1:3] + [meta_values[4]]))  # pkey
        meta_values.append("_".join(meta_values[1:5]))  # collection_key
        metadata = FileMetadata(*meta_values)
    except (ValueError, IndexError, TypeError) as e:
        msg = f"Could not determine metadata from file {collection_file.stem}"
        raise ApplicationError(msg) from e
    else:
        return metadata


def _mask_file_collection_key(collection_file: Path) -> Path:
    """Return a file with a masked collection key stem.

    Args:
        collection_file (Path): The path of the file to mask.

    Returns:
        Path: masked collection key file path.
    """
    metadata = _metadata_from_filename(collection_file)
    return Path(
        f"{collection_file.with_name(collection_file.stem.replace(metadata.collection_key, fake.collection_key))}{collection_file.suffix}"
    )


def _find_collections_to_process(search_path: Path) -> List[Path]:
    """Find collections to process.

    Args:
        search_path (Path): The path to search for collections.

        Check for zip and gzip files and return list of paths.

    Returns:
        List[Path]: list of files found matching the collection pattern.
    """
    file_list = (
        set(search_path.glob("opdb_oracle_*.zip"))
        .union(set(search_path.glob("opdb_oracle_*.gz")))
        .difference(
            set(search_path.glob("opdb_oracle_*ERROR.zip")).union(set(search_path.glob("opdb_oracle_*ERROR.gz")))
        )
    )
    return list(file_list)


def _extract_collection(collection_file: Path, extract_path: Path) -> None:
    """Extract collection.

    Args:
        collection_file (Path): The collection file to extract.
        extract_path (Path): The past to extract the collection to.
    """
    logger.info("Processing %s", collection_file)
    if str(collection_file).endswith("zip"):
        with zf.ZipFile(collection_file, "r") as f:
            f.extractall(path=extract_path)
    elif str(collection_file).endswith("gz"):
        with tarfile.open(collection_file) as tf:
            tf.extractall(extract_path)


def _find_files_referenced_by_rule(rule: DataMaskRule, search_path: Path) -> Path | None:
    """Find files referenced by data masking rule.

    Args:
        rule (DataMaskRule): The data masking rule to apply.
        search_path (Path): The search path to look for collection csv files.

    Raises:
        ApplicationError: Raised when no matching files are found.
        ApplicationError: Raise when more than 1 matching files are found if no wildcard search.

    Returns:
       Optional[list[Path]]: If the files were found, the files.  Else None.
    """
    matched_file = list(
        set(search_path.glob(f"opdb__{rule.table_name}__*.csv")).difference(search_path.glob("opdb__defines__*.csv"))
    )
    if not matched_file and not rule.missing_ok:
        msg = f"Could not find a file to match for {rule.table_name}"
        raise ApplicationError(msg)
    if matched_file and not rule.missing_ok and len(matched_file) == 0:
        msg = f"Could not find a file to match for {rule.table_name}"
        raise ApplicationError(msg)
    if matched_file and len(matched_file) > 1 and rule.table_name != "*":
        msg = f"Found too many files when searching for {rule.table_name}.  Found {matched_file}"
        raise ApplicationError(msg)
    if matched_file and len(matched_file) != 0:
        return matched_file  # type: ignore[return-value]
    return None


def _generate_identity_map_from_rule(
    rule: DataMaskRule,
    collection_file: Path,
    existing_identity_map: Dict[str, str] | None = None,
) -> Dict[str, str]:
    """Generate identity map from rule.

    This function takes a rule and identifies all of the unique values that should be replaced within a collection.

    It returns a dictionary where the original value is the key and the replacement is the value.

    Args:
        rule (DataMaskRule): Data masking rule
        collection_file (Path): Collection file to use
        existing_identity_map (Optional[Dict[str, str]], optional): Pass in an existing identity map from a previous rule. Defaults to None.

    Returns:
        Dict[str, str]: A dictionary of the original and replacement values.
    """
    if existing_identity_map is None:
        existing_identity_map = {}

    all_values: List[str] = list(existing_identity_map.keys())
    collection_metadata = _metadata_from_filename(collection_file)
    column_position = rule.column_position
    if Version(collection_metadata.script_version) < Version("4.3.15") and rule.column_position < -2:
        column_position += 2
    with collection_file.open(mode="r", encoding="utf-8") as f:
        data_to_mask = csv.reader(f, delimiter="|", quotechar='"')
        for rn, row in enumerate(data_to_mask):
            if rn > 0:
                # Get the value from column to mask
                all_values.append(row[column_position])
    unique_values = list(dict.fromkeys(all_values))
    return {
        value: existing_identity_map.get(
            value,
            rule.fake_function(),
        )
        for value in unique_values
    }


def _apply_identity_map_to_file(rule: DataMaskRule, collection_file: Path, identity_map: Dict[str, str]) -> None:
    """Apply Identity Map to File.

    This function takes the identity map (a dictionary that holds unique values and their masked replacement value) and applies it to a file.

    This replaced data is written to a tempfile that replaces the original file on success.

    Args:
        rule (DataMaskRule): The current masking rule
        collection_file (Path): the current collection file to mask
        identity_map (Dict[str, str]): The key/replacement key dictionary.
    """
    with (
        collection_file.open(mode="r", encoding="utf-8") as f,
        NamedTemporaryFile(mode="w", delete=False, encoding="utf-8") as t,
    ):
        data_to_mask = csv.reader(f, delimiter="|", quotechar='"')
        temp_file = csv.writer(t, delimiter="|", quotechar='"')
        collection_metadata = _metadata_from_filename(collection_file)
        column_position = rule.column_position
        if Version(collection_metadata.script_version) < Version("4.3.15") and rule.column_position < -2:
            column_position += 2
        for rn, row in enumerate(data_to_mask):
            if rn > 0:
                row[column_position] = identity_map.get(row[column_position], "~~UNMAPPED~~")
            temp_file.writerow(row)
        shutil.move(t.name, collection_file)


def _package_collection(
    collection: Path,
    identity_map: Dict[str, str],
    extract_path: Path,
    output_path: Path,
) -> Tuple[Path, Path]:
    """Packages de-identified files.

    Args:
        collection (Path): the collection archive
        identity_map (Dict[str, str]): the map of unique keys to the replacement value
        extract_path (Path): The path to look for extracted files.
        output_path (Path): The path to write the new collection.

    Returns:
        Tuple[Path, Path]: The new key file and archive file.
    """
    masked_collection_file = _mask_file_collection_key(collection)
    identity_map[collection.name] = masked_collection_file.name

    logger.info("Zipping %s", masked_collection_file.stem)
    archive_file = Path(output_path / f"{masked_collection_file.stem}.zip")
    key_file = Path(collection.parent / f"{masked_collection_file.stem}.key")
    matched_files = (
        list(extract_path.glob("*.csv")) + list(extract_path.glob("*.log")) + list(extract_path.glob("*.txt"))
    )
    with zf.ZipFile(archive_file, "w", compression=zf.ZIP_DEFLATED, compresslevel=9) as f:
        for matched_file in matched_files:
            masked_file = _mask_file_collection_key(matched_file)
            identity_map[matched_file.name] = masked_file.name
            f.write(matched_file, f"{masked_file.stem}{masked_file.suffix}")
    with key_file.open(mode="w", encoding="utf-8") as f:
        f.write(
            f"Use this file to map the masked values to the original values in collection {archive_file.stem}.zip\n"
        )
        f.write("ORIGINAL,MASKED\n")
        csv_writer = csv.writer(f, delimiter="|", quotechar='"')
        csv_writer.writerows(sorted(identity_map.items()))
    return key_file, archive_file


def _clean_folder(extract_path: Path) -> None:
    """Clean temp folder

    Args:
        extract_path (Path): The path to clean.
    """
    logger.info("Cleaning temporary files")
    matched_files = list(extract_path.glob("*"))
    for file in matched_files:
        if file.exists() and file.is_file():
            file.unlink()
        if file.exists() and file.is_dir():
            shutil.rmtree(file, ignore_errors=True)


def main() -> None:
    """Google Database Migration Assessment - Collection Masking Script."""

    def _validate_collection_path(path: str) -> str:
        p = Path(path)
        if p.exists() and p.is_dir() and path is not None:
            return path
        msg = f"collection-path {path} is not a valid path"
        raise argparse.ArgumentTypeError(msg)

    parser = argparse.ArgumentParser(description="Google Database Migration Assessment - Collection Masking Script")
    parser.add_argument(
        "--verbose",
        "-v",
        action="count",
        help="Logging level: 0: ERROR, 1: INFO, 2: DEBUG",
        default=1,
    )
    parser.add_argument(
        "--collection-path",
        type=_validate_collection_path,
        help="Path to search for collections.",
    )
    parser.add_argument(
        "--output-path",
        default=str(Path.cwd()),
        help="Path to write masked collections.",
    )
    args = parser.parse_args(args=None if sys.argv[1:] else ["--help"])
    if Path(args.collection_path) == Path(args.output_path):
        msg = "output-path must not be the same path as collection-path"
        raise argparse.ArgumentTypeError(msg)
    level = args.verbose
    if level == 0:
        level = logging.ERROR
    elif level == 1:
        level = logging.INFO
    else:
        level = logging.DEBUG

    logging.basicConfig(
        level=level,
        format="%(asctime)s [%(levelname)8s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    logger.info("Starting Collection De-Identification Process.")
    run_masker(Path(args.collection_path), Path(args.output_path))


if __name__ == "__main__":
    main()
