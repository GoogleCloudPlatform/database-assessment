#!/usr/bin/env python3
# Copyright 2024 Google LLC

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     https://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from __future__ import annotations

import argparse
import csv
import logging
import random
import shutil
import string
import sys
import tarfile
import zipfile as zf
from dataclasses import dataclass, field
from enum import Flag, auto
from pathlib import Path
from tempfile import NamedTemporaryFile, TemporaryDirectory
from typing import Any, Dict, List, Tuple
from collections import Counter

from packaging.version import Version
from packaging.version import parse as parse_version

__all__ = (
    "ApplicationError",
    "DataMaskRule",
    "main",
    "run_masker",
)

__version__ = "4.3.45"

logger = logging.getLogger(__name__)

here = Path(__file__).parent


@dataclass
class DataMaskRule:
    mask_type: DataMaskTypes
    table_name: str
    column_position: int
    missing_ok: bool
    fake_function: Any
    row_filter: List[str] = field(default_factory=list)


@dataclass
class FileMetadata:
    script_version: str
    host_name: str
    database_name: str
    instance_name: str
    date_time: str
    pkey: str
    collection_key: str
    database_type: str


class DMAFaker:
    """Fake data provider to provide masked values"""

    hostname_prefixes: List[str] = [
        "db",
        "srv",
        "desktop",
        "laptop",
        "lt",
        "email",
        "web",
    ]

    hostname_domains: List[str] = [
        "com",
        "com",
        "com",
        "com",
        "com",
        "com",
        "biz",
        "info",
        "net",
        "org",
    ]

    def __init__(self) -> None:
        self._collection_key: str
        self._date_time: str
        self._dbid: str = self._digits(length=10)

    def _letters(self, length: int) -> str:
        return "".join(random.choices(string.ascii_lowercase, k=length))

    def _digits(self, length: int) -> str:
        return "".join(random.choices(string.digits, k=length))

    @property
    def date_time(self) -> str:
        return self._date_time

    @date_time.setter
    def date_time(self, value: str) -> None:
        self._date_time = value

    @property
    def collection_key(self) -> str:
        return self._collection_key

    def set_collection_key_from_date_time(self, date_time: str) -> None:
        if not self._date_time:
            raise ValueError("date_time is not set")
        self._collection_key = f"{self.hostname()}_{self.database_name()}_{self.instance_name()}_{date_time}"

    def database_name(self) -> str:
        return f"""db-{self._letters(length=8)}""".upper()

    def instance_name(self) -> str:
        return f"""inst-{self._letters(length=8)}""".upper()

    def hostname(self) -> str:
        prefix = f"""{random.choice(self.hostname_prefixes)}-{self._digits(length=2)}"""
        domain = f"""{self._letters(length=5)}.{random.choice(self.hostname_domains)}"""
        return f"{prefix}.{domain}"

    def pkey(self) -> str:
        return f"{self.hostname()}_{self.database_name()}_{self.date_time}"

    def dma_source_id(self) -> str:
        return f"{self.hostname()}_{self.instance_name()}_{self._dbid}"

    def user_name(self) -> str:
        return f"USER_{self._letters(length=6)}{self._digits(length=3)}".upper()

    def object_name(self) -> str:
        return f"OBJ_{self._letters(length=12)}{self._digits(length=3)}".upper()

    def parameter_value(self) -> str:
        return f"PARAM_{self._letters(length=12)}{self._digits(length=3)}".upper()

fake = DMAFaker()


class DataMaskTypes(Flag):
    SCHEMA_NAME = auto()
    PKEY = auto()
    DMA_SOURCE_ID = auto()
    INSTANCE_NAME = auto()
    HOST_NAME = auto()
    DATABASE_NAME = auto()
    OBJECT_NAME = auto()
    PARAMETER_VALUE = auto()

oracle_params_to_mask: List[str] = [
    'cdb_cluster_name',
    'dg_broker_config_file1',
    'dg_broker_config_file2',
    'diagnostic_dest',
    'background_dump_dest',
    'user_dump_dest',
    'core_dump_dest',
    'audit_file_dest',
    'db_name',
    'db_unique_name',
    'db_domain',
    'instance_name',
    'dispatchers',
    'local_listener',
    'remote_listener',
    'connection_brokers',
    'log_archive_config',
    'db_create_file_dest',
    'db_create_online_log_dest_1',
    'db_recovery_file_dest',
    'undo_tablespace',
    'log_archive_dest_3',
    'log_archive_dest_4',
    'service_names',
    'control_files',
    'log_archive_dest_1',
    'spfile',
    'db_domain',
    'spfile',
    'service_names',
    'control_files',
    'log_archive_dest_1',
    'log_archive_dest_3',
    'log_archive_dest_4',
    'log_archive_config',
    'db_create_file_dest',
    'db_create_online_log_dest_1',
    'db_recovery_file_dest',
    'undo_tablespace',
    'db_domain',
    'instance_name',
    'dispatchers',
    'local_listener',
    'remote_listener',
    'connection_brokers',
    'background_dump_dest',
    'user_dump_dest',
    'core_dump_dest',
    'audit_file_dest',
    'db_name',
    'db_unique_name',
    'dg_broker_config_file1',
    'dg_broker_config_file2',
    'diagnostic_dest',
    'cdb_cluster_name',
    'db_domain'
]

oracle_schemas_to_skip: List[str] = [
    'ANONYMOUS',
    'APPQOSSYS',
    'AUDSYS',
    'AURORA$JIS$UTILITY$',
    'AURORA$ORB$UNAUTHENTICATED',
    'CSMIG',
    'CTXSYS',
    'CTXSYS',
    'DBA_ADM',
    'DBMS_PRIVILEGE_CAPTURE',
    'DBSFWUSER',
    'DBSNMP',
    'DIP',
    'DMSYS',
    'DVF',
    'DVSYS',
    'EXFSYS',
    'GSMADMIN_INTERNAL',
    'GSMCATUSER',
    'GSMUSER',
    'LBACSYS',
    'MDDATA',
    'MDSYS',
    'MGDSYS',
    'MGMT_VIEW',
    'MGMT_VIEW',
    'MTMSYS',
    'ODM',
    'ODM_MTR',
    'OJVMSYS',
    'OLAPSYS',
    'ORACLE_OCM',
    'ORDDATA',
    'ORDPLUGINS',
    'ORDSYS',
    'OSE$HTTP$ADMIN',
    'OUTLN',
    'OWBSYS',
    'OWBSYS_AUDIT',
    'PERFSTAT',
    'SDE',
    'SI_INFORMTN_SCHEMA',
    'SPATIAL_CSW_ADMIN_USR',
    'SPATIAL_WFS_ADMIN_USR',
    'SQLTXADMIN',
    'SQLTXPLAIN',
    'STRMADMIN',
    'SYS',
    'SYSBACKUP',
    'SYSDG',
    'SYSKM',
    'SYSMAN',
    'SYSTEM',
    'TRACESRV',
    'TSMSYS',
    'WEBSYS',
    'WKPROXY',
    'WKSYS',
    'WK_PROXY',
    'WK_TEST',
    'WMSYS',
    'XDB',
    'XS$NULL'
]


DATA_MASK_CONFIG_MSSQL = [
    DataMaskRule( mask_type=DataMaskTypes.PKEY, table_name="*", column_position=0, missing_ok=False, fake_function=fake.pkey),
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="BlockFeatures",               column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="ColumnDatatypes",             column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="CompInstalled",               column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="DatabaseLevelBlockFeatures",  column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="DbClusterNodes",              column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="DbMachineSpecs",              column_position=  1, missing_ok=True,  fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="DbServerConfig",              column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="DbServerFlags",               column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="DbSizes",                     column_position=  4, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="DbccTrace",                   column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="DiskVolInfo",                 column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="DmvPerfmon",                  column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="IndexList",                   column_position= 18, missing_ok=True,  fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="LinkedSrvrs",                 column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="ObjectList",                  column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="PerfCounter",                 column_position= 25, missing_ok=True,  fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="PerfMonData",                 column_position= 25, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="ServerProps",                 column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="TableList",                   column_position= 17, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="TranLogBkupCountByHourByDay", column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="TranLogBkupSizeByHourByDay",  column_position= -2, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="UserConnections",             column_position= 18, missing_ok=False, fake_function=fake.dma_source_id,),   # dma_source_id
    # Columns other than dma_source_id
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="ColumnDatatypes",             column_position= 1,  missing_ok=True,  fake_function=fake.database_name,),    # database_name
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="ColumnDatatypes",             column_position= 2,  missing_ok=True,  fake_function=fake.user_name,),        # schema_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="ColumnDatatypes",             column_position= 3,  missing_ok=True,  fake_function=fake.object_name,),      # table_name
    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="CompInstalled",               column_position= 1,  missing_ok=True,  fake_function=fake.hostname,),         # physical_server_name
    DataMaskRule( mask_type=DataMaskTypes.INSTANCE_NAME, table_name="CompInstalled",               column_position= 2,  missing_ok=True,  fake_function=fake.instance_name,),    # sql_instance_name
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="DatabaseLevelBlockFeatures",  column_position= 1,  missing_ok=True,  fake_function=fake.database_name,),    # database_name
    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="DbClusterNodes",              column_position= 1,  missing_ok=True,  fake_function=fake.hostname,),         # node_name
    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="DbMachineSpecs",              column_position= 3,  missing_ok=True,  fake_function=fake.hostname,),         # MachineName
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="DbSizes",                     column_position= 1,  missing_ok=True,  fake_function=fake.database_name,),    # database_name
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="IndexList",                   column_position= 1,  missing_ok=True,  fake_function=fake.database_name,),    # database_name
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="IndexList",                   column_position= 2,  missing_ok=True,  fake_function=fake.user_name,),        # schema_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="IndexList",                   column_position= 3,  missing_ok=True,  fake_function=fake.object_name,),      # table_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="IndexList",                   column_position= 4,  missing_ok=True,  fake_function=fake.object_name,),      # index_name
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="LinkedSrvrs",                 column_position= 1,  missing_ok=False, fake_function=fake.database_name,),    # name
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="LinkedSrvrs",                 column_position= 4,  missing_ok=False, fake_function=fake.database_name,),    # data_source
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="ObjectList",                  column_position= 1,  missing_ok=True,  fake_function=fake.database_name,),    # database_name
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="ObjectList",                  column_position= 2,  missing_ok=True,  fake_function=fake.user_name,),        # schema_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="ObjectList",                  column_position= 3,  missing_ok=True,  fake_function=fake.object_name,),      # object_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="ObjectList",                  column_position= 8,  missing_ok=True,  fake_function=fake.object_name,),      # associated_table_name
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="PerfCounter",                 column_position= 1,  missing_ok=True,  fake_function=fake.database_name,),    # database_name
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="TableList",                   column_position= 1,  missing_ok=True,  fake_function=fake.database_name,),    # database_name
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="TableList",                   column_position= 2,  missing_ok=True,  fake_function=fake.user_name,),        # schema_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="TableList",                   column_position= 3,  missing_ok=True,  fake_function=fake.object_name,),      # table_name
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="UserConnections",             column_position= 1,  missing_ok=True,  fake_function=fake.database_name,),    # database_name
    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="UserConnections",             column_position= 3,  missing_ok=True,  fake_function=fake.hostname,),         # host_name
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="UserConnections",             column_position= 4,  missing_ok=True,  fake_function=fake.user_name),         # program_name
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="UserConnections",             column_position= 5,  missing_ok=True,  fake_function=fake.user_name),         # login_name
    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="UserConnections",             column_position=14,  missing_ok=False, fake_function=fake.hostname,),         # nt_domain
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="UserConnections",             column_position=15,  missing_ok=False, fake_function=fake.user_name,),        # nt_user_name
    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="ServerProps",                 column_position= 2,  missing_ok=False, fake_function=fake.hostname, row_filter=["ServerName"]),         # Special for ServerPrps entries
    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="ServerProps",                 column_position= 2,  missing_ok=False, fake_function=fake.hostname, row_filter=["MachineName"]),        # Special for ServerPrps entries
]

DATA_MASK_CONFIG_ORACLE = [
    DataMaskRule(
        mask_type=DataMaskTypes.PKEY, table_name="*", column_position=0, missing_ok=False, fake_function=fake.pkey
    ),
    DataMaskRule(
        mask_type=DataMaskTypes.DMA_SOURCE_ID, table_name="*", column_position=-2, missing_ok=False, fake_function=fake.dma_source_id
    ),
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="columntypes",      column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="compressbytype",   column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="datatypes",        column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="dbobjectnames",    column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="dbobjects",        column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="dblinks",          column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="dtlsourcecode",    column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="exttab",           column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="indextypedtl",     column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="indextypes",       column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="lobsizing",        column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="mviewtypes",       column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="sourcecode",       column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="tableconstraints", column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="tablesnopk",       column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="tabletypedtl",     column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="tabletypes",       column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="triggers",         column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="usedspacedetails", column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="usrsegatt",        column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # owner

    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="columntypes",      column_position= 3,  missing_ok=True, fake_function=fake.object_name,),  # table_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="dbobjectnames",    column_position= 3,  missing_ok=True, fake_function=fake.object_name,),  # object_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="dtlsourcecode",    column_position= 3,  missing_ok=True, fake_function=fake.object_name,),  # object_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="indextypedtl",     column_position= 12, missing_ok=True, fake_function=fake.object_name,),  # table_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="indextypedtl",     column_position= 13, missing_ok=True, fake_function=fake.object_name,),  # index_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="lobsizing",        column_position= 3,  missing_ok=True, fake_function=fake.object_name,),  # table_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="lobsizing",        column_position= 7,  missing_ok=True, fake_function=fake.object_name,),  # column_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="lobsizing",        column_position= 9,  missing_ok=True, fake_function=fake.object_name,),  # table_partition_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="lobsizing",        column_position= 12, missing_ok=True, fake_function=fake.object_name,),  # lob_partition_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="lobsizing",        column_position= 15, missing_ok=True, fake_function=fake.object_name,),  # table_subpartition_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="lobsizing",        column_position= 18, missing_ok=True, fake_function=fake.object_name,),  # lob_subpartition_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="lobsizing",        column_position= 20, missing_ok=True, fake_function=fake.object_name,),  # lob_segment_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="lobsizing",        column_position= 21, missing_ok=True, fake_function=fake.object_name,),  # segment_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="lobsizing",        column_position= 22, missing_ok=True, fake_function=fake.object_name,),  # segment_partition_name

    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="opkeylog",         column_position= 3, missing_ok=True, fake_function=fake.hostname,),      # host_name
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="opkeylog",         column_position= 4, missing_ok=True, fake_function=fake.database_name,), # db_name
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="opkeylog",         column_position= 5, missing_ok=True, fake_function=fake.database_name,), # instance_name
    DataMaskRule( mask_type=DataMaskTypes.SCHEMA_NAME,   table_name="users",            column_position= 2, missing_ok=True, fake_function=fake.user_name,),     # username
    DataMaskRule( mask_type=DataMaskTypes.INSTANCE_NAME, table_name="dbinstances",      column_position= 2, missing_ok=True, fake_function=fake.instance_name,), # instance_name
    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="dbinstances",      column_position= 3, missing_ok=True, fake_function=fake.hostname,),      # host_name
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="dbsummary",        column_position= 2, missing_ok=True, fake_function=fake.database_name,), # db_name
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="dbsummary",        column_position=-3, missing_ok=True, fake_function=fake.database_name,), # db_unique_name
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="pdbsinfo",         column_position= 3, missing_ok=True, fake_function=fake.database_name, row_filter=["CDB$ROOT"]), # pdb_name
    DataMaskRule( mask_type=DataMaskTypes.DATABASE_NAME, table_name="pdbsopenmode",     column_position= 2, missing_ok=True, fake_function=fake.database_name,), # name
    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="sourceconn",       column_position= 4, missing_ok=True, fake_function=fake.hostname,),      # program
    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="sourceconn",       column_position= 5, missing_ok=True, fake_function=fake.hostname,),      # module_name
    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="sourceconn",       column_position= 6, missing_ok=True, fake_function=fake.hostname,),      # machine_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="tableconstraints", column_position= 3, missing_ok=True, fake_function=fake.object_name,),   # table_name
    DataMaskRule( mask_type=DataMaskTypes.OBJECT_NAME,   table_name="tabletypedtl",     column_position= 3, missing_ok=True, fake_function=fake.object_name,),   # table_name
    DataMaskRule( mask_type=DataMaskTypes.HOST_NAME,     table_name="DbMachineSpecs",   column_position= 3, missing_ok=True, fake_function=fake.hostname,),      # MachineName
    DataMaskRule( mask_type=DataMaskTypes.PARAMETER_VALUE, table_name="dbparameters",   column_position= 4, missing_ok=True, fake_function=fake.parameter_value, row_filter=oracle_params_to_mask),   # value
    DataMaskRule( mask_type=DataMaskTypes.PARAMETER_VALUE, table_name="dbparameters",   column_position= 5, missing_ok=True, fake_function=fake.parameter_value, row_filter=oracle_params_to_mask),   # default_value
]


class ApplicationError(Exception):
    """Application Error

    Raised when any error occurs with the masking process.
    """

def _exclude_owner_from_masking(owner_name: str) -> bool:
    """
    Certain built-in schemas are filtered out in ETL, so we need exclude them from masking.

    :param owner_name: The name of the owner/schema being checked
    :type owner_name: str
    :return: Returns TRUE if the owner/schema should be excluded from masking.
    :rtype: bool
    """

    return (owner_name is not None) and ((owner_name in oracle_schemas_to_skip) or ( owner_name.startswith(("APEX_", "FLOWS_","OPS$"))))


def _generate_and_apply_identity_map(identity_map: dict[str, str], rule: DataMaskRule, collection_files: Path) -> dict[str, str]:
    for collection_file in collection_files:  # type: ignore[attr-defined]
        while True:
            identity_map = _generate_identity_map_from_rule(rule, collection_file, existing_identity_map=identity_map)
            duplicate_masks = _check_for_duplicate_values(existing_identity_map=identity_map)
            if len(duplicate_masks) == 0:
                break
            logger.info(F"Found %s duplicate mask values, reprocessing file...", len(duplicate_masks))
            for dup_val in duplicate_masks:
                identity_map.pop(dup_val, None)

        _apply_identity_map_to_file(rule, collection_file, identity_map)
    return identity_map

def run_masker(input_dir: Path | None = None, output_path: Path | None = None) -> None:
    """Run Masker.

    Args:
        input_dir (Optional[Path]): Path containing collections to mask. Defaults to None.
        output_path (Optional[Path]): Path to write masked collection. Defaults to None.
    """
    with TemporaryDirectory() as temp_dir:
        work_dir = Path(temp_dir)
        work_dir.mkdir(exist_ok=True)
        input_dir = input_dir or Path("input")
        output_path = output_path or Path("masked")

        for dir_name in (input_dir, output_path):
            dir_name.mkdir(parents=True, exist_ok=True)

        collections = _find_collections_to_process(input_dir)
        if len(collections) == 0:
            logger.info("No collections found in location %s", input_dir)
            logger.info("Exiting...")
            sys.exit(1)

        for collection in collections:
            collection_metadata = _metadata_from_filename(collection)
            fake.date_time = collection_metadata.date_time
            fake.set_collection_key_from_date_time(collection_metadata.date_time)
            identity_map: dict[str, str] = {collection_metadata.pkey: fake.pkey()}
            _extract_collection(collection, work_dir)
            if collection_metadata.database_type == 'mssql':
                data_mask_config = DATA_MASK_CONFIG_MSSQL
            elif collection_metadata.database_type == 'oracle':
                data_mask_config = DATA_MASK_CONFIG_ORACLE
            else:
                msg = f"Unmapped database type {collection_metadata.database_type}."
                raise ApplicationError(msg)

            for rule in data_mask_config:
                if rule.mask_type == DataMaskTypes.DMA_SOURCE_ID and Version(
                    collection_metadata.script_version
                ) < Version("4.3.15"):
                    continue
                collection_files = _find_files_referenced_by_rule(rule, work_dir)
                if collection_files:
                    identity_map = _generate_and_apply_identity_map(identity_map, rule, collection_files)

            logger.info("Completed work on %s", collection.stem)
            _collection_key_file, _new_collection_archive = _package_collection(
                collection, identity_map, work_dir, output_path
            )
            _clean_folder(work_dir)

        logger.info("------------------------------------------------------------------------------------------")
        logger.info("------------------------------------------------------------------------------------------")
        logger.info(
            "Masking complete.  Please submit the files in the '%s' directory.",
            output_path,
        )
        logger.info(
            "Retain the *key files in the '%s' directory to map data from the assessment report to the original values.",
            input_dir,
        )


def _metadata_from_filename(collection_file: Path) -> FileMetadata:
    """Return metadata by parsing the collection file name.

    Args:
        collection_file (Path): The path of the file to read.

    Raises:
        ApplicationError: Raised when parsing metadata fails.

    Returns:
        FileMetadata: namedtuple containing metadata attributes.
    """
    try:
        meta_values = collection_file.stem.split("__", maxsplit=2)[-1].split("_")[1:6]
        meta_values[0] = str(parse_version(meta_values[0]))  # type: ignore[call-overload]
        meta_values.append("_".join([*meta_values[1:3], meta_values[4]]))  # pkey
        meta_values.append("_".join(meta_values[1:5]))  # collection_key
        meta_values.append(collection_file.stem.split(sep="_")[1]) # database_type
        metadata = FileMetadata(*meta_values)
    except (ValueError, IndexError, TypeError) as e:
        msg = f"Could not determine metadata from file {collection_file.stem}"
        raise ApplicationError(msg) from e
    else:
        return metadata


def _mask_file_collection_key(collection_file: Path) -> Path:
    """Return a file with a masked collection key stem.

    Args:
        collection_file (Path): The path of the file to mask.

    Returns:
        Path: masked collection key file path.
    """
    metadata = _metadata_from_filename(collection_file)
    return Path(
        f"{collection_file.with_name(collection_file.stem.replace(metadata.collection_key, fake.collection_key))}{collection_file.suffix}"
    )


def _find_collections_to_process(search_path: Path) -> List[Path]:
    """Find collections to process.

    Args:
        search_path (Path): The path to search for collections.

        Check for zip and gzip files and return list of paths.

    Returns:
        List[Path]: list of files found matching the collection pattern.
    """
    file_list = (
        set(search_path.glob("opdb_oracle_*.zip"))
        .union(set(search_path.glob("opdb_oracle_*.gz")))
        .union(set(search_path.glob("opdb_mssql_*.zip")))
        .difference(
            set(search_path.glob("opdb_oracle_*ERROR.zip")).union(set(search_path.glob("opdb_oracle_*ERROR.gz"))).union(set(search_path.glob("opdb_mssql_*ERROR.zip")))
        )
    )
    return list(file_list)


def _extract_collection(collection_file: Path, extract_path: Path) -> None:
    """Extract collection.

    Args:
        collection_file (Path): The collection file to extract.
        extract_path (Path): The past to extract the collection to.
    """
    logger.info("Processing %s", collection_file)
    if str(collection_file).endswith("zip"):
        with zf.ZipFile(collection_file, "r") as f:
            f.extractall(path=extract_path)
    elif str(collection_file).endswith("gz"):
        with tarfile.open(collection_file) as tf:
            tf.extractall(extract_path)


def _find_files_referenced_by_rule(rule: DataMaskRule, search_path: Path) -> Path | None:
    """Find files referenced by data masking rule.

    Args:
        rule (DataMaskRule): The data masking rule to apply.
        search_path (Path): The search path to look for collection csv files.

    Raises:
        ApplicationError: Raised when no matching files are found.
        ApplicationError: Raise when more than 1 matching files are found if no wildcard search.

    Returns:
       Optional[list[Path]]: If the files were found, the files.  Else None.
    """
    matched_file = list(
        set(search_path.glob(f"opdb__{rule.table_name}__*.csv")).difference(search_path.glob("opdb__defines__*.csv"))
    )
    if not matched_file and not rule.missing_ok:
        msg = f"Could not find a file to match for {rule.table_name}"
        raise ApplicationError(msg)
    if matched_file and not rule.missing_ok and len(matched_file) == 0:
        msg = f"Could not find a file to match for {rule.table_name}"
        raise ApplicationError(msg)
    if matched_file and len(matched_file) > 1 and rule.table_name != "*":
        msg = f"Found too many files when searching for {rule.table_name}.  Found {matched_file}"
        raise ApplicationError(msg)
    if matched_file and len(matched_file) != 0:
        return matched_file  # type: ignore[return-value]
    return None


def _check_for_duplicate_values(
    existing_identity_map: Dict[str, str] ,
) -> Dict[str, str]:
    """
    Check the dictionary for duplicate values in case there was a collision on the randomizer.

    This function finds any values assigned to more than one key and returns the set.

    Args:
        existing_identity_map ([Dict[str, str]]): Pass in an existing identity map.

    Returns:
        Dict[str, str]: A dictionary of keys with duplicate values.
    """

    duplicate_count = Counter(existing_identity_map.values())

    duplicate_masks = {k: v for k, v in existing_identity_map.items() if duplicate_count[v] > 1}
    if len(duplicate_masks) > 0:
        logger.info(f"Duplicate masking values detected : %s", duplicate_masks)
    return duplicate_masks



def _generate_identity_map_from_rule(
    rule: DataMaskRule,
    collection_file: Path,
    existing_identity_map: Dict[str, str] | None = None,
) -> Dict[str, str]:
    """Generate identity map from rule.

    This function takes a rule and identifies all of the unique values that should be replaced within a collection.

    It returns a dictionary where the original value is the key and the replacement is the value.

    Args:
        rule (DataMaskRule): Data masking rule
        collection_file (Path): Collection file to use
        existing_identity_map (Optional[Dict[str, str]], optional): Pass in an existing identity map from a previous rule. Defaults to None.

    Returns:
        Dict[str, str]: A dictionary of the original and replacement values.
    """
    if existing_identity_map is None:
        existing_identity_map = {}

    all_values: List[str] = list(existing_identity_map.keys())
    collection_metadata = _metadata_from_filename(collection_file)
    column_position = rule.column_position
    if Version(collection_metadata.script_version) < Version("4.3.15") and rule.column_position < -2:
        column_position += 2
    with collection_file.open(mode="r", encoding="utf-8") as f:
        data_to_mask = csv.reader(f, delimiter="|", quotechar='"')
        for rn, row in enumerate(data_to_mask):
            if (rn > 0) and (len(row) > 0):
                if (rule.mask_type == DataMaskTypes.DATABASE_NAME) and (row[column_position] in { 'master', 'model', 'msdb', 'tempdb', 'PDB$SEEED', 'CDB$ROOT' }) :
                    continue
                if (rule.mask_type == DataMaskTypes.SCHEMA_NAME) and (_exclude_owner_from_masking(row[column_position])):
                    continue
                # Get the value from column to mask
                if (rule.row_filter is None) or (len(rule.row_filter)==0):
                    all_values.append(row[column_position])
                elif any(check_val in rule.row_filter for check_val in row):
                    all_values.append(row[column_position])

    unique_values = list(dict.fromkeys(all_values))
    return {
        value: existing_identity_map.get(
            value,
            rule.fake_function(),
        )
        for value in unique_values
    }


def _apply_identity_map_to_file(rule: DataMaskRule, collection_file: Path, identity_map: Dict[str, str]) -> None:
    """Apply Identity Map to File.

    This function takes the identity map (a dictionary that holds unique values and their masked replacement value) and applies it to a file.

    This replaced data is written to a tempfile that replaces the original file on success.

    Args:
        rule (DataMaskRule): The current masking rule
        collection_file (Path): the current collection file to mask
        identity_map (Dict[str, str]): The key/replacement key dictionary.
    """
    with (
        collection_file.open(mode="r", encoding="utf-8") as f,
        NamedTemporaryFile(mode="w", delete=False, encoding="utf-8") as t,
    ):
        data_to_mask = csv.reader(f, delimiter="|", quotechar='"')
        temp_file = csv.writer(t, delimiter="|", quotechar='"')
        collection_metadata = _metadata_from_filename(collection_file)
        column_position = rule.column_position
        if Version(collection_metadata.script_version) < Version("4.3.15") and rule.column_position < -2:
            column_position += 2
        for rn, row in enumerate(data_to_mask):
            if (rn > 0)  and (len(row) > 0) and not ((rule.mask_type == DataMaskTypes.DATABASE_NAME) and (row[column_position] in { 'master', 'model', 'msdb', 'tempdb', 'PDB$SEED', 'CDB$ROOT'} )):
                if (rule.row_filter is None) or (len(rule.row_filter) == 0):
                    prefix = "BIN$" if row[column_position].startswith("BIN$") else ""
                    if not _exclude_owner_from_masking(row[column_position]):
                        row[column_position] = prefix + identity_map.get(row[column_position], "~~UNMAPPED~~")
                elif any(check_val in rule.row_filter for check_val in row):
                    row[column_position] = identity_map.get(row[column_position], "~~UNMAPPED~~")

            temp_file.writerow(row)
        shutil.move(t.name, collection_file)


def _rewrite_mssql_collector_log(logfile_name: Path, zipfile_name: str) -> None :
    """Rewrites the collector log file to include only the masked zipfile name.

    Args:
        logfile_name (Path): The name of the mssql collector log file.
        zipfile_name (str): The name of the output zip file.
    """
    with open(logfile_name, "w", encoding="utf-8") as newlog:
        newlog.write(f"[00/00/00 00:00:00]   DMA_MASKER applied, request original logfile for troubleshooting.\n")
        newlog.write(f"[00/00/00 00:00:00]   Zipping Output to {zipfile_name} ...\n")


def _rewrite_oracle_collector_log(logfile_name: Path, zipfile_name: str) -> None :
    """Rewrites the collector log file to include only the masked zipfile name.

    Args:
        logfile_name (Path): The name of the mssql collector log file.
        zipfile_name (str): The name of the output zip file.
    """
    with open(logfile_name, "w", encoding="utf-8") as newlog:
        newlog.write(f"NOTE: DMA_MASKER applied, request original logfile for troubleshooting.\n")
        newlog.write(f"ZIPFILE:  {zipfile_name}\n")



def _package_collection(
    collection: Path,
    identity_map: Dict[str, str],
    extract_path: Path,
    output_path: Path,
) -> Tuple[Path, Path]:
    """Packages de-identified files.

    Args:
        collection (Path): the collection archive
        identity_map (Dict[str, str]): the map of unique keys to the replacement value
        extract_path (Path): The path to look for extracted files.
        output_path (Path): The path to write the new collection.

    Returns:
        Tuple[Path, Path]: The new key file and archive file.
    """
    masked_collection_file = _mask_file_collection_key(collection)
    identity_map[collection.name] = masked_collection_file.name

    logger.info("Zipping %s", masked_collection_file.stem)
    archive_file = Path(output_path / f"{masked_collection_file.stem}.zip")
    key_file = Path(collection.parent / f"{masked_collection_file.stem}.key")
    matched_files = (
        list(extract_path.glob("*.csv")) + list(extract_path.glob("*.log")) + list(extract_path.glob("*.txt"))
    )
    with zf.ZipFile(archive_file, "w", compression=zf.ZIP_DEFLATED, compresslevel=9) as f:
        for matched_file in matched_files:
            # Exclude the Permon log fileq
            if "opdb__perfMonLog_" not in str(matched_file):
                masked_file = _mask_file_collection_key(matched_file)
                identity_map[matched_file.name] = masked_file.name

                if ("opdb_mssql_collectorLog" in str(matched_file)):
                    _rewrite_mssql_collector_log(matched_file, masked_collection_file.stem + ".zip")

                if ("opdb__defines" in str(matched_file)):
                    _rewrite_oracle_collector_log(matched_file, masked_collection_file.stem + ".zip")

                if (( "opdb_mssql_sqlErrorlog" not in str(matched_file)) and ("opdb__manifest" not in str(matched_file))):
                    f.write(matched_file, f"{masked_file.stem}{masked_file.suffix}")

    with key_file.open(mode="w", encoding="utf-8") as f:
        f.write(
            f"Use this file to map the masked values to the original values in collection {archive_file.stem}.zip\n"
        )
        f.write("ORIGINAL,MASKED\n")
        csv_writer = csv.writer(f, delimiter="|", quotechar='"')
        csv_writer.writerows(sorted(identity_map.items()))
    return key_file, archive_file


def _clean_folder(extract_path: Path) -> None:
    """Clean temp folder

    Args:
        extract_path (Path): The path to clean.
    """
    logger.info("Cleaning temporary files")
    matched_files = list(extract_path.glob("*"))
    for file in matched_files:
        if file.exists() and file.is_file():
            file.unlink()
        if file.exists() and file.is_dir():
            shutil.rmtree(file, ignore_errors=True)


def main() -> None:
    """Google Database Migration Assessment - Collection Masking Script."""

    def _validate_collection_path(path: str) -> str:
        p = Path(path)
        if p.exists() and p.is_dir() and path is not None:
            return path
        msg = f"collection-path {path} is not a valid path"
        raise argparse.ArgumentTypeError(msg)

    parser = argparse.ArgumentParser(description="Google Database Migration Assessment - Collection Masking Script")
    parser.add_argument(
        "--verbose",
        "-v",
        action="count",
        help="Logging level: 0: ERROR, 1: INFO, 2: DEBUG",
        default=1,
    )
    parser.add_argument(
        "--collection-path",
        type=_validate_collection_path,
        help="Path to search for collections.",
    )
    parser.add_argument(
        "--output-path",
        default=str(Path.cwd()),
        help="Path to write masked collections.",
    )
    args = parser.parse_args(args=None if sys.argv[1:] else ["--help"])
    if Path(args.collection_path) == Path(args.output_path):
        msg = "output-path must not be the same path as collection-path"
        raise argparse.ArgumentTypeError(msg)
    level = args.verbose
    if level == 0:
        level = logging.ERROR
    elif level == 1:
        level = logging.INFO
    else:
        level = logging.DEBUG

    logging.basicConfig(
        level=level,
        format="%(asctime)s [%(levelname)8s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    logger.info("Starting Collection De-Identification Process.")
    run_masker(Path(args.collection_path), Path(args.output_path))


if __name__ == "__main__":
    main()
